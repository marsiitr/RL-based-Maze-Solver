{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Q-Learning (Reinforcement Learning) based Maze Solver**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Required Libraries & Modules:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import numpy as np\r\n",
    "import random\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from ImgPreprocess import preprocess"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent of the Q-Learning Model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class Agent:\r\n",
    "    # defining Agent related functions and instances\r\n",
    "\r\n",
    "    def __init__(self, i = 0, j = 0):\r\n",
    "        # initailises the Agent object with passed coordinates (default: (0, 0))\r\n",
    "\r\n",
    "        self.i = i\r\n",
    "        self.j = j\r\n",
    "\r\n",
    "    @property\r\n",
    "    def loc(self):\r\n",
    "        # returns the current location of the Agent in the maze as tuple\r\n",
    "\r\n",
    "        return (self.i, self.j)\r\n",
    "    \r\n",
    "    def vmove(self, direction):\r\n",
    "        # returns a new object of the Agent after moving vertically \r\n",
    "\r\n",
    "        direction = 1 if direction > 0 else -1\r\n",
    "        return Agent(self.i + direction, self.j)\r\n",
    "    \r\n",
    "    def hmove(self, direction):\r\n",
    "        # returns a new object of the Agent after moving horizontally\r\n",
    "\r\n",
    "        direction = 1 if direction > 0 else -1\r\n",
    "        return Agent(self.i, direction + self.j) \r\n",
    "    \r\n",
    "    def __repr__(self):\r\n",
    "        # returns a string denoting the current location of the Agent in the maze\r\n",
    "        \r\n",
    "        return str(self.loc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining the Maze: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class Maze:\r\n",
    "    # general maze class defination and related functions and instances\r\n",
    "\r\n",
    "    def __init__(self, rows = 4, columns = 4, l = [0, 0, -1, -1], aspect = 1, reward = 0):\r\n",
    "        # initializing a matrix of order (rows x columns) (default: 4x4) with 0s\r\n",
    "        # mousy is an object of Agent class \r\n",
    "        # mousy starts at cell (x0, y0) (default: top-left corner)\r\n",
    "        # end point is at cell (x1, y1) (default: bottom-right corner)\r\n",
    "        \r\n",
    "        self.r = rows\r\n",
    "        self.c = columns\r\n",
    "        if reward == 0: # reward is proportional to the size of the maze\r\n",
    "            self.reward = 10 * rows * columns \r\n",
    "        else: self.reward = reward # manually set\r\n",
    "        self.env = np.zeros((rows, columns))\r\n",
    "        self.x0, self.y0, self.x1, self.y1 = l\r\n",
    "        self.mousy = Agent(self.x0, self.y0) # defining the start cell\r\n",
    "\r\n",
    "        self.env[self.x1, self.y1] = 1 # defining the end cell\r\n",
    "        if self.x0 < 0: self.x0 = self.x0 % rows\r\n",
    "        if self.y0 < 0: self.y0 = self.y0 % columns\r\n",
    "        if self.x1 < 0: self.x1 = self.x1 % rows\r\n",
    "        if self.y1 < 0: self.y1 = self.y1 % columns\r\n",
    "            \r\n",
    "        self.trace = [] # for tracking path of agent\r\n",
    "        self.aspect = aspect # aspect ratio of Matplotlib plots (default: 1)\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        # resetting mousy to original coordinates (x0, y0) \r\n",
    "\r\n",
    "        self.mousy.i = self.x0\r\n",
    "        self.mousy.j = self.y0\r\n",
    "        self.trace = []\r\n",
    "\r\n",
    "    def state_for_agent(self):\r\n",
    "        # indexing each grid cell of maze as a unique state with unique index \r\n",
    "        # and returning the index to current position of mousy\r\n",
    "\r\n",
    "        a = self.mousy\r\n",
    "        return a.i * self.c + a.j\r\n",
    "        \r\n",
    "    def in_bounds(self, i, j):\r\n",
    "        # checks if the coordinates are outside the maze \r\n",
    "\r\n",
    "        return i >= 0 and i < self.r and j >= 0 and j < self.c\r\n",
    "    \r\n",
    "    def agent_in_bounds(self, a):\r\n",
    "        # checks if the agent is inside the maze or not\r\n",
    "\r\n",
    "        return self.in_bounds(a.i, a.j)\r\n",
    "    \r\n",
    "    def agent_dient(self, a):\r\n",
    "        # checks if the coordinates are not prohibited/ are not walls\r\n",
    "\r\n",
    "        return self.env[a.i, a.j] != -1\r\n",
    "    \r\n",
    "    def is_valid_new_agent(self, a):\r\n",
    "        # checks if the agent state is valid, i.e. niether outside maze nor on walls \r\n",
    "\r\n",
    "        return self.agent_in_bounds(a) and self.agent_dient(a)\r\n",
    "    \r\n",
    "    @property\r\n",
    "    def all_actions(self):  \r\n",
    "        # returns all the actions that mousy can take (both valid or invalid) as a \r\n",
    "        # list of objects of Agent class\r\n",
    "\r\n",
    "        a = self.mousy    \r\n",
    "        return [\r\n",
    "            a.vmove(1),  # down\r\n",
    "            a.vmove(-1), # up\r\n",
    "            a.hmove(1),  # right\r\n",
    "            a.hmove(-1), # left\r\n",
    "        ]\r\n",
    "       \r\n",
    "    def compute_possible_moves(self):\r\n",
    "        # returns all the valid actions that mousy can take as a list of tuples containing  \r\n",
    "        # object of Agent class and its index\r\n",
    "\r\n",
    "        moves = self.all_actions\r\n",
    "        return [(m, ii) for ii, m in enumerate(moves) if self.is_valid_new_agent(m)]\r\n",
    "\r\n",
    "    def has_won(self):\r\n",
    "        # returns if mousy has won (reached the end of the matrix)\r\n",
    "\r\n",
    "        a = self.mousy\r\n",
    "        return self.env[a.i, a.j] == 1\r\n",
    "            \r\n",
    "    def do_a_move(self,a):\r\n",
    "        # moves mousy to a new valid position on the maze, updates the trace and returns \r\n",
    "        # the corresponding score: 10 for winning else -0.1\r\n",
    "\r\n",
    "        if self.is_valid_new_agent(a): \r\n",
    "            self.trace.append(self.mousy.loc) # Update trace\r\n",
    "            self.mousy = a\r\n",
    "            return self.reward if self.has_won() else -0.1\r\n",
    "        else: \r\n",
    "            self.show_trace()\r\n",
    "            print(a.i, a.j)\r\n",
    "            assert False, \"Mousy can't go there\"\r\n",
    "        \r\n",
    "    def visualise(self):\r\n",
    "        # prints the entire maze with the agent's location\r\n",
    "        # 0's denote valid cells, -1's denote walls \r\n",
    "        # 1 denotes the end of the maze and 6 denotes the current position of agent\r\n",
    "\r\n",
    "        assert self.agent_in_bounds(self.mousy), \"mousy is out of bounds\"\r\n",
    "        ec = self.env.copy()\r\n",
    "        mo = self.mousy\r\n",
    "        ec[mo.i,mo.j] = 6\r\n",
    "        print(ec)\r\n",
    "    \r\n",
    "    def show_trace(self):\r\n",
    "        # shows a pictorial representation of path trace using Matplotlib\r\n",
    "        # aspect defines the height to width ratio of the pixels\r\n",
    "        \r\n",
    "        ec = self.env.copy()\r\n",
    "        mo = self.mousy\r\n",
    "        for x in self.trace: ec[x[0], x[1]] = -0.5\r\n",
    "        ec[mo.i, mo.j] = 0.5\r\n",
    "        plt.imshow(ec, aspect = self.aspect)\r\n",
    "        plt.clim(-1, 1)\r\n",
    "        plt.colorbar()\r\n",
    "        plt.show()\r\n",
    "        \r\n",
    "    def show_maze(self):\r\n",
    "        # shows a pictorial representation of the maze using Matplotlib\r\n",
    "        # aspect defines the height to width ratio of the pixels\r\n",
    "\r\n",
    "        ec = self.env.copy()\r\n",
    "        mo = self.mousy\r\n",
    "        ec[mo.i,mo.j] = 0.5\r\n",
    "        plt.imshow(ec, aspect = self.aspect)\r\n",
    "        plt.clim(-1, 1)\r\n",
    "        plt.colorbar()\r\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q-Learning Model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class Qlearning:\r\n",
    "    # implements the Reinforcement Learning (Q-Learning) algorithm using Bellman Equation\r\n",
    "\r\n",
    "    def __init__(self, num_states, num_actions, lr = 0.1, df = 0.99):\r\n",
    "        # initializing the learning rate(a) with lr(default: 0.1), the discount factor(g) \r\n",
    "        # with df(default: 0.99) and the q-matrix(q) of order (num_states x num_actions) \r\n",
    "        # with 0s\r\n",
    "        \r\n",
    "        self.q = np.zeros((num_states, num_actions))\r\n",
    "        self.a = lr\r\n",
    "        self.g = df\r\n",
    "    \r\n",
    "    def update(self, st, at, rt, st1):\r\n",
    "        # updates the q-matrix using Bellman Equation\r\n",
    "\r\n",
    "        q = self.q  \r\n",
    "        a = self.a\r\n",
    "        g = self.g\r\n",
    "        q[st, at] = (1-a) * q[st, at] + a * (rt + g * np.max(q[st1]))\r\n",
    "        \r\n",
    "    def __str__(self):\r\n",
    "        # Returns the q-matrix as string\r\n",
    "        \r\n",
    "        rows, cols = self.q.shape\r\n",
    "        s = \"\"\r\n",
    "        for r in range(rows):\r\n",
    "            for c in range(cols):\r\n",
    "                s += str(self.q[r, c]) + \", \"\r\n",
    "            s += '\\n'\r\n",
    "\r\n",
    "        return s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Maze Generator:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def make_test_maze(l, s = 4): \r\n",
    "    # returns a random s x s test maze which the algorithm solves using RL\r\n",
    "    # default size: 4x4\r\n",
    "\r\n",
    "    m = Maze(s, s, l)\r\n",
    "    e = m.env\r\n",
    "\r\n",
    "    for i in range(len(e)):\r\n",
    "        for j in range(len(e[i])):\r\n",
    "            # randomly converts about 35% of maze cells to walls \r\n",
    "            # except for start and end of the maze\r\n",
    "\r\n",
    "            if (i == m.x0 and j == m.y0) or (i == m.x1 and j == m.y1) : continue\r\n",
    "            if random.random() < 0.35:\r\n",
    "                e[i, j] = -1\r\n",
    "    return m"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def final(M, q):\r\n",
    "    print(\"----------------------BEST SOLUTION OF THE MAZE----------------------\")\r\n",
    "    M.reset()\r\n",
    "    M.show_maze()\r\n",
    "    while not M.has_won():\r\n",
    "        s = M.state_for_agent()\r\n",
    "        a_idx = np.argmax(q[s])\r\n",
    "        M.do_a_move(M.all_actions[a_idx])\r\n",
    "        M.show_trace()\r\n",
    "        \r\n",
    "    M.visualise()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Driver Code:\n",
    "* ### _Exploration_ :\n",
    "    Randomly travelling the maze and updating the q-matrix\n",
    "* ### _Exploitation_ :\n",
    "    Using the currently available q-matrix to travel along the path of maximum reward\n",
    "### Random maze generate:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def main1():\r\n",
    "    # DRIVER CODE\r\n",
    "\r\n",
    "    s = int(input(\"Enter the size of the maze:\"))\r\n",
    "    q = Qlearning(s**2, 4)\r\n",
    "\r\n",
    "    l = list(map(int, input(\"Enter space-separated coordinates of start and end points, Press 'd' for default values: \").split()))\r\n",
    "\r\n",
    "    if l[0] == 'd' : l = [0, 0, -1, -1]\r\n",
    "    \r\n",
    "    # Flag boolean value, remains 'False' until a solvable maze is generated\r\n",
    "    go_ahead = False \r\n",
    "    \r\n",
    "    # generating random mazes and manually checking if solvable\r\n",
    "    while not go_ahead:\r\n",
    "        M = make_test_maze(l, s)\r\n",
    "        M.show_maze()\r\n",
    "        ctu = input (\"Press 'n' to generate a new maze else hit 'Enter':\")\r\n",
    "        if ctu.lower() == 'n':\r\n",
    "            continue\r\n",
    "        go_ahead = True\r\n",
    "    \r\n",
    "    # running the algorithm for 500 episodes to generate a q-matrix\r\n",
    "    # to get the most efficient solution\r\n",
    "    for i in range(500):\r\n",
    "        # Maze reset\r\n",
    "        M.reset()  \r\n",
    "        final_score = 0\r\n",
    "        \r\n",
    "        itr = 0\r\n",
    "        while not M.has_won():\r\n",
    "            # iterating until mousy has won in each episode\r\n",
    "            # simulataneously updating the q-matrix\r\n",
    "            itr += 1\r\n",
    "            st = M.state_for_agent() # current state of mousy\r\n",
    "                   \r\n",
    "            if random.random() > 0.7  or i < 150:\r\n",
    "                # exploration: for the first 150 episodes and 30% of the remaining \r\n",
    "                # 350 on average\r\n",
    "                moves = M.compute_possible_moves()\r\n",
    "                random.shuffle(moves) # randomly choosing from the available moves\r\n",
    "                move, move_idx = moves[0]\r\n",
    "\r\n",
    "            else:\r\n",
    "                # exploitation: for 70% of the remaining episodes after first 50 \r\n",
    "                # on average\r\n",
    "                moves = M.all_actions\r\n",
    "                \r\n",
    "                # sorting the q-values in descending order\r\n",
    "                q_val = q.q[st] \r\n",
    "                y = [x for x in enumerate(q_val)]\r\n",
    "                y.sort(key = lambda p: p[1], reverse = True) \r\n",
    "                                \r\n",
    "                # choosing valid new move with maximum reward\r\n",
    "                for z in y:\r\n",
    "                    move_idx = z[0]\r\n",
    "                    move = moves[move_idx]\r\n",
    "                    if M.is_valid_new_agent(move): break\r\n",
    "\r\n",
    "            at = move_idx\r\n",
    "            score = M.do_a_move(move)                     \r\n",
    "            final_score += score \r\n",
    "            rt = score\r\n",
    "            st1 = M.state_for_agent()\r\n",
    "\r\n",
    "            q.update(st, at , rt, st1) # updating the q-matrix after a move\r\n",
    "        \r\n",
    "        print(f\"finished episode {i} with a final score of {final_score} and in {itr} iterations\")\r\n",
    "        print(\"END\")\r\n",
    "\r\n",
    "    print(\"----------------------Q-MATRIX----------------------\\n\", q) # the final q-matrix\r\n",
    "                \r\n",
    "    # printing the most efficient solution\r\n",
    "    final(M, q.q)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Image input maze generate:\n",
    "\n",
    "'ep_part' ought to be an iterable like tuple or list.\n",
    "* the first element defines the total number of episodes\n",
    "* the second element defines the initial number of episodes for which the agent only explores the maze\n",
    "* the third element is a fraction, defining the part of the remaining episodes for which the agent exploits the maze"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def main2(s, margin = 0.01, way = None, pix = 1, div = 128, aspect = 1, ep_part = [500, 150, 0.7], reward = 0):\r\n",
    "    # DRIVER CODE\r\n",
    "    \r\n",
    "    p = preprocess(s)\r\n",
    "    p.generate(margin, way, pix, div)\r\n",
    "    q = Qlearning(p.h * p.w, 4)\r\n",
    "    M = Maze(p.h, p.w, p.loc, aspect)\r\n",
    "    M.env = p.nim\r\n",
    "    M.show_maze()\r\n",
    "    \r\n",
    "    # running the algorithm for specified number of episodes to generate \r\n",
    "    # a q-matrix to get the most efficient solution\r\n",
    "    for i in range(ep_part[0]):\r\n",
    "        # Maze reset\r\n",
    "        M.reset()  \r\n",
    "        final_score = 0\r\n",
    "        \r\n",
    "        itr = 0\r\n",
    "        while not M.has_won():\r\n",
    "            # iterating until mousy has won in each episode\r\n",
    "            # simulataneously updating the q-matrix\r\n",
    "            itr += 1\r\n",
    "            st = M.state_for_agent() # current state of mousy\r\n",
    "                   \r\n",
    "            if random.random() > ep_part[2]  or i < ep_part[1]:\r\n",
    "                # exploration\r\n",
    "                moves = M.compute_possible_moves()\r\n",
    "                random.shuffle(moves) # randomly choosing from the available moves\r\n",
    "                move, move_idx = moves[0]\r\n",
    "\r\n",
    "            else:\r\n",
    "                # exploitation\r\n",
    "                moves = M.all_actions\r\n",
    "                \r\n",
    "                # sorting the q-values in descending order\r\n",
    "                q_val = q.q[st]\r\n",
    "                y = [x for x in enumerate(q_val)]\r\n",
    "                y.sort(key = lambda p: p[1], reverse = True) \r\n",
    "                                \r\n",
    "                # choosing valid new move with maximum reward\r\n",
    "                for z in y:\r\n",
    "                    move_idx = z[0]\r\n",
    "                    move = moves[move_idx]\r\n",
    "                    if M.is_valid_new_agent(move): break\r\n",
    "\r\n",
    "            at = move_idx\r\n",
    "            score = M.do_a_move(move)                     \r\n",
    "            final_score += score \r\n",
    "            rt = score\r\n",
    "            st1 = M.state_for_agent()\r\n",
    "\r\n",
    "            q.update(st, at , rt, st1) # updating the q-matrix after a move\r\n",
    "        \r\n",
    "        print(f\"finished episode {i} with a final score of {final_score} and in {itr} iterations\")\r\n",
    "        print(\"END\")\r\n",
    "\r\n",
    "    print(\"----------------------Q-MATRIX----------------------\\n\", q) # the final q-matrix\r\n",
    "                \r\n",
    "    # printing the most efficient solution\r\n",
    "    final(M, q.q)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd4c67ef142469da7dc4d338a32ac40116904d26076b8e6aa587d80720bc6a2b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}